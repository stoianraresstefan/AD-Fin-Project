{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepCAR Experiment: ALPIN-Enhanced DeepAR Forecasting\n",
    "\n",
    "This notebook implements and compares **baseline DeepAR** vs **ALPIN-enhanced DeepAR** using the **BatchCP** method from the DeepCAR paper.\n",
    "\n",
    "## Background: DeepCAR (Changepoint-Aware DeepAR)\n",
    "\n",
    "The DeepCAR paper proposes a simple but effective approach to improve probabilistic forecasting:\n",
    "\n",
    "1. **Problem**: Standard DeepAR training uses all available data, including windows that span regime changes (changepoints). Training on these \"contaminated\" batches teaches the model incorrect temporal patterns.\n",
    "\n",
    "2. **Solution - BatchCP**: Filter out training batches whose encoder windows contain or overlap with detected changepoints. This ensures the model only learns from \"clean\" homogeneous segments.\n",
    "\n",
    "3. **Key Insight**: By using ALPIN for accurate changepoint detection, we can identify and exclude problematic training samples, leading to better forecast accuracy.\n",
    "\n",
    "## Experiment Goal\n",
    "\n",
    "Compare forecast accuracy (MAE/RMSE) between:\n",
    "- **Baseline DeepAR**: Trained on all data\n",
    "- **ALPIN-Enhanced DeepAR**: Trained with BatchCP filtering\n",
    "\n",
    "We expect the ALPIN-enhanced version to produce better forecasts, especially on signals with clear regime changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "**Important**: Before running this notebook, ensure dependencies are installed:\n",
    "\n",
    "```bash\n",
    "uv sync\n",
    "```\n",
    "\n",
    "This will install `pytorch-forecasting`, `pytorch-lightning`, and `torch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Iterator, Any\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "# PyTorch Forecasting\n",
    "from pytorch_forecasting import TimeSeriesDataSet, DeepAR\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAE, RMSE\n",
    "\n",
    "# ALPIN\n",
    "from alpin import ALPIN\n",
    "from alpin.data.synthetic import generate_synthetic_signals, alpin_signals_to_deepar_df\n",
    "from alpin.metrics import evaluate_all\n",
    "from alpin.visualization import plot_signal\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "pl.seed_everything(SEED)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch Lightning version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Define hyperparameters for the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "N_SIGNALS = 10          # Number of synthetic signals\n",
    "N_SAMPLES = 500         # Samples per signal\n",
    "NOISE_STD = 1.0         # Noise standard deviation\n",
    "\n",
    "# DeepAR configuration\n",
    "MAX_ENCODER_LENGTH = 60   # Context window\n",
    "MAX_PREDICTION_LENGTH = 20  # Forecast horizon\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Training configuration\n",
    "MAX_EPOCHS = 20\n",
    "LEARNING_RATE = 1e-3\n",
    "HIDDEN_SIZE = 32\n",
    "RNN_LAYERS = 2\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# BatchCP configuration\n",
    "CP_TOLERANCE = 2  # Safety margin around changepoints\n",
    "\n",
    "# Train/Val split\n",
    "TRAIN_RATIO = 0.8\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Signals: {N_SIGNALS} x {N_SAMPLES} samples\")\n",
    "print(f\"  Encoder length: {MAX_ENCODER_LENGTH}\")\n",
    "print(f\"  Prediction length: {MAX_PREDICTION_LENGTH}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Max epochs: {MAX_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Synthetic Data\n",
    "\n",
    "We generate piecewise constant signals with known changepoints using ALPIN's synthetic data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic signals with ground truth changepoints\n",
    "signals, changepoints = generate_synthetic_signals(\n",
    "    n_signals=N_SIGNALS,\n",
    "    n_samples=N_SAMPLES,\n",
    "    noise_std=NOISE_STD,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(signals)} signals, each with {N_SAMPLES} samples\")\n",
    "print(f\"Changepoints per signal: {[len(cp) for cp in changepoints]}\")\n",
    "print(f\"Total changepoints: {sum(len(cp) for cp in changepoints)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DeepAR-compatible DataFrame\n",
    "df = alpin_signals_to_deepar_df(signals, changepoints)\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Series IDs: {df['series_id'].unique().tolist()[:5]}...\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/validation by time\n",
    "train_cutoff = int(N_SAMPLES * TRAIN_RATIO)\n",
    "\n",
    "# For time series, we split by time index\n",
    "train_df = df[df['time_idx'] < train_cutoff].copy()\n",
    "val_df = df[df['time_idx'] >= train_cutoff - MAX_ENCODER_LENGTH].copy()  # Include encoder context\n",
    "\n",
    "print(f\"Train samples: {len(train_df)} (time_idx < {train_cutoff})\")\n",
    "print(f\"Val samples: {len(val_df)} (time_idx >= {train_cutoff - MAX_ENCODER_LENGTH})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize example signals with changepoints\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    signal = signals[i]\n",
    "    cps = changepoints[i]\n",
    "    \n",
    "    ax.plot(signal, color='#2C3E50', linewidth=1.2, alpha=0.8, label='Signal')\n",
    "    \n",
    "    for j, cp in enumerate(cps):\n",
    "        label = 'Changepoint' if j == 0 else None\n",
    "        ax.axvline(x=cp, color='#E74C3C', linestyle='--', linewidth=2, alpha=0.7, label=label)\n",
    "    \n",
    "    # Mark train/val split\n",
    "    ax.axvline(x=train_cutoff, color='#27AE60', linestyle='-', linewidth=2, alpha=0.9, label='Train/Val Split')\n",
    "    \n",
    "    ax.set_ylabel(f'Signal {i}')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_title(f'Series {i}: {len(cps)} changepoints', fontweight='bold')\n",
    "\n",
    "axes[-1].set_xlabel('Time Index')\n",
    "plt.suptitle('Synthetic Signals with Ground Truth Changepoints', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ALPIN Changepoint Detection\n",
    "\n",
    "Train ALPIN on training data to detect changepoints. These detections will be used for BatchCP filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only training portion of signals for ALPIN training\n",
    "train_signals = [s[:train_cutoff] for s in signals]\n",
    "train_changepoints = [[cp for cp in cps if cp < train_cutoff] for cps in changepoints]\n",
    "\n",
    "# Train ALPIN model\n",
    "alpin_model = ALPIN()\n",
    "alpin_model.fit(train_signals, train_changepoints)\n",
    "\n",
    "print(f\"ALPIN learned optimal beta: {alpin_model.beta_opt:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict changepoints on all signals (full length for completeness)\n",
    "detected_changepoints = {}\n",
    "for i, signal in enumerate(signals):\n",
    "    series_id = f\"series_{i}\"\n",
    "    detected = alpin_model.predict(signal)\n",
    "    detected_changepoints[series_id] = detected\n",
    "\n",
    "print(\"Detected changepoints per series:\")\n",
    "for sid, cps in detected_changepoints.items():\n",
    "    print(f\"  {sid}: {cps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ALPIN detection quality\n",
    "print(\"\\nALPIN Detection Metrics:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "all_metrics = []\n",
    "for i in range(N_SIGNALS):\n",
    "    series_id = f\"series_{i}\"\n",
    "    detected = detected_changepoints[series_id]\n",
    "    ground_truth = changepoints[i]\n",
    "    \n",
    "    metrics = evaluate_all(detected, ground_truth, N_SAMPLES, tolerance=10)\n",
    "    all_metrics.append(metrics)\n",
    "    \n",
    "    print(f\"Series {i}: Precision={metrics['precision']:.2f}, Recall={metrics['recall']:.2f}\")\n",
    "\n",
    "# Average metrics\n",
    "avg_metrics = pd.DataFrame(all_metrics).mean()\n",
    "print(\"-\" * 50)\n",
    "print(f\"Average: Precision={avg_metrics['precision']:.2f}, Recall={avg_metrics['recall']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ALPIN detection on one signal\n",
    "example_idx = 0\n",
    "plot_signal(\n",
    "    signals[example_idx],\n",
    "    true_changepoints=changepoints[example_idx],\n",
    "    pred_changepoints=detected_changepoints[f'series_{example_idx}'],\n",
    "    title=f'ALPIN Detection on Series {example_idx}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline DeepAR Training\n",
    "\n",
    "Train DeepAR on all available training data without any filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TimeSeriesDataSet for training\n",
    "training = TimeSeriesDataSet(\n",
    "    train_df,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"value\",\n",
    "    group_ids=[\"series_id\"],\n",
    "    max_encoder_length=MAX_ENCODER_LENGTH,\n",
    "    max_prediction_length=MAX_PREDICTION_LENGTH,\n",
    "    time_varying_unknown_reals=[\"value\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    target_normalizer=GroupNormalizer(groups=[\"series_id\"]),\n",
    ")\n",
    "\n",
    "# Create validation dataset from training parameters\n",
    "validation = TimeSeriesDataSet.from_dataset(training, val_df, stop_randomization=True)\n",
    "\n",
    "print(f\"Training samples: {len(training)}\")\n",
    "print(f\"Validation samples: {len(validation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=BATCH_SIZE, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=BATCH_SIZE, num_workers=0)\n",
    "\n",
    "print(f\"Train batches: {len(train_dataloader)}\")\n",
    "print(f\"Val batches: {len(val_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline DeepAR model\n",
    "baseline_deepar = DeepAR.from_dataset(\n",
    "    training,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    rnn_layers=RNN_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    log_interval=10,\n",
    "    log_val_interval=1,\n",
    "    reduce_on_plateau_patience=3,\n",
    ")\n",
    "\n",
    "print(f\"Baseline DeepAR parameters: {sum(p.numel() for p in baseline_deepar.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline model\n",
    "baseline_trainer = pl.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=50,  # Limit for quick demo\n",
    "    limit_val_batches=20,\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=False,\n",
    "    logger=False,\n",
    ")\n",
    "\n",
    "print(\"Training Baseline DeepAR...\")\n",
    "baseline_trainer.fit(\n",
    "    baseline_deepar,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")\n",
    "print(\"Baseline training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate baseline predictions\n",
    "baseline_predictions = baseline_deepar.predict(val_dataloader, return_y=True, mode=\"prediction\")\n",
    "\n",
    "# Extract predictions and actuals\n",
    "baseline_preds = baseline_predictions.output\n",
    "baseline_actuals = baseline_predictions.y[0]\n",
    "\n",
    "print(f\"Baseline predictions shape: {baseline_preds.shape}\")\n",
    "print(f\"Baseline actuals shape: {baseline_actuals.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate baseline metrics\n",
    "def calculate_forecast_metrics(predictions: torch.Tensor, actuals: torch.Tensor) -> Dict[str, float]:\n",
    "    \"\"\"Calculate MAE and RMSE for forecasts.\"\"\"\n",
    "    preds = predictions.cpu().numpy().flatten()\n",
    "    actual = actuals.cpu().numpy().flatten()\n",
    "    \n",
    "    mae = np.mean(np.abs(preds - actual))\n",
    "    rmse = np.sqrt(np.mean((preds - actual) ** 2))\n",
    "    \n",
    "    return {'MAE': mae, 'RMSE': rmse}\n",
    "\n",
    "baseline_metrics = calculate_forecast_metrics(baseline_preds, baseline_actuals)\n",
    "print(\"Baseline DeepAR Metrics:\")\n",
    "print(f\"  MAE:  {baseline_metrics['MAE']:.4f}\")\n",
    "print(f\"  RMSE: {baseline_metrics['RMSE']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot baseline forecast example\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "# Get first batch for visualization\n",
    "sample_idx = 0\n",
    "pred_sample = baseline_preds[sample_idx].cpu().numpy()\n",
    "actual_sample = baseline_actuals[sample_idx].cpu().numpy()\n",
    "\n",
    "time_axis = np.arange(len(actual_sample))\n",
    "\n",
    "ax.plot(time_axis, actual_sample, 'o-', color='#2E86AB', label='Actual', markersize=4)\n",
    "ax.plot(time_axis, pred_sample, 's-', color='#E74C3C', label='Baseline Forecast', markersize=4)\n",
    "\n",
    "ax.set_xlabel('Forecast Horizon')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('Baseline DeepAR: Example Forecast vs Actual', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, linestyle=':', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ALPIN-Enhanced DeepAR (BatchCP)\n",
    "\n",
    "Implement the BatchCP filtering method and train DeepAR with changepoint-aware batch selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChangePointAwareDataLoader:\n",
    "    \"\"\"\n",
    "    Wrapper that filters out batches containing changepoints in encoder window.\n",
    "    \n",
    "    Implements the BatchCP method from DeepCAR: batches where the encoder window\n",
    "    overlaps with a detected changepoint are skipped during training.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataloader : DataLoader\n",
    "        Original PyTorch DataLoader from TimeSeriesDataSet\n",
    "    changepoints_dict : Dict[str, List[int]]\n",
    "        Mapping from series_id to list of changepoint indices\n",
    "    encoder_length : int\n",
    "        Length of the encoder window\n",
    "    tolerance : int\n",
    "        Safety margin around changepoints\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dataloader: DataLoader,\n",
    "        changepoints_dict: Dict[str, List[int]],\n",
    "        encoder_length: int,\n",
    "        tolerance: int = 2\n",
    "    ):\n",
    "        self.dataloader = dataloader\n",
    "        self.changepoints_dict = changepoints_dict\n",
    "        self.encoder_length = encoder_length\n",
    "        self.tolerance = tolerance\n",
    "        self.filtered_count = 0\n",
    "        self.total_count = 0\n",
    "        \n",
    "    def _batch_contains_changepoint(self, batch: tuple) -> bool:\n",
    "        \"\"\"\n",
    "        Check if any sample in batch has a changepoint in its encoder window.\n",
    "        \n",
    "        Args:\n",
    "            batch: Tuple of (x_dict, y_tuple) from TimeSeriesDataSet\n",
    "            \n",
    "        Returns:\n",
    "            True if batch should be filtered out\n",
    "        \"\"\"\n",
    "        x_dict, y = batch\n",
    "        \n",
    "        # Get encoder time indices - these tell us the time range of each sample\n",
    "        # encoder_time_idx has shape (batch_size, encoder_length)\n",
    "        if 'encoder_time_idx' in x_dict:\n",
    "            encoder_times = x_dict['encoder_time_idx']\n",
    "        else:\n",
    "            # Fallback: use relative time index if available\n",
    "            encoder_times = x_dict.get('time_idx', None)\n",
    "            if encoder_times is None:\n",
    "                return False  # Cannot determine, don't filter\n",
    "        \n",
    "        # Get series groups to identify which series each sample belongs to\n",
    "        groups = x_dict.get('groups', None)\n",
    "        if groups is None:\n",
    "            return False  # Cannot determine series, don't filter\n",
    "        \n",
    "        batch_size = encoder_times.shape[0]\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Get time range of this sample's encoder window\n",
    "            sample_times = encoder_times[i].cpu().numpy()\n",
    "            start_time = int(sample_times.min())\n",
    "            end_time = int(sample_times.max())\n",
    "            \n",
    "            # Get series ID for this sample\n",
    "            series_idx = int(groups[i, 0].item())  # First group dimension is series\n",
    "            series_id = f\"series_{series_idx}\"\n",
    "            \n",
    "            # Check if any changepoint falls within encoder window\n",
    "            if series_id in self.changepoints_dict:\n",
    "                for cp in self.changepoints_dict[series_id]:\n",
    "                    # Check if changepoint (with tolerance) overlaps encoder window\n",
    "                    if (start_time - self.tolerance) <= cp <= (end_time + self.tolerance):\n",
    "                        return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def __iter__(self) -> Iterator:\n",
    "        \"\"\"Iterate over batches, skipping those with changepoints.\"\"\"\n",
    "        self.filtered_count = 0\n",
    "        self.total_count = 0\n",
    "        \n",
    "        for batch in self.dataloader:\n",
    "            self.total_count += 1\n",
    "            \n",
    "            if self._batch_contains_changepoint(batch):\n",
    "                self.filtered_count += 1\n",
    "                continue  # Skip this batch\n",
    "            \n",
    "            yield batch\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return length of underlying dataloader (upper bound).\"\"\"\n",
    "        return len(self.dataloader)\n",
    "    \n",
    "    def get_filtering_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics about batch filtering.\"\"\"\n",
    "        return {\n",
    "            'total_batches': self.total_count,\n",
    "            'filtered_batches': self.filtered_count,\n",
    "            'kept_batches': self.total_count - self.filtered_count,\n",
    "            'filter_ratio': self.filtered_count / max(self.total_count, 1)\n",
    "        }\n",
    "\n",
    "print(\"ChangePointAwareDataLoader class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filtered dataloader for ALPIN-enhanced training\n",
    "filtered_train_dataloader = ChangePointAwareDataLoader(\n",
    "    dataloader=train_dataloader,\n",
    "    changepoints_dict=detected_changepoints,\n",
    "    encoder_length=MAX_ENCODER_LENGTH,\n",
    "    tolerance=CP_TOLERANCE\n",
    ")\n",
    "\n",
    "print(f\"Created ChangePointAwareDataLoader\")\n",
    "print(f\"  Detected changepoints: {sum(len(cps) for cps in detected_changepoints.values())}\")\n",
    "print(f\"  Encoder length: {MAX_ENCODER_LENGTH}\")\n",
    "print(f\"  Tolerance: {CP_TOLERANCE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ALPIN-enhanced DeepAR model (same architecture as baseline)\n",
    "alpin_deepar = DeepAR.from_dataset(\n",
    "    training,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    rnn_layers=RNN_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    log_interval=10,\n",
    "    log_val_interval=1,\n",
    "    reduce_on_plateau_patience=3,\n",
    ")\n",
    "\n",
    "print(f\"ALPIN-Enhanced DeepAR parameters: {sum(p.numel() for p in alpin_deepar.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training loop with filtered dataloader\n",
    "# Note: Since PyTorch Lightning trainer expects a standard dataloader,\n",
    "# we'll do a manual training loop here for the filtered version\n",
    "\n",
    "print(\"Training ALPIN-Enhanced DeepAR with BatchCP filtering...\")\n",
    "\n",
    "optimizer = torch.optim.Adam(alpin_deepar.parameters(), lr=LEARNING_RATE)\n",
    "alpin_deepar.train()\n",
    "\n",
    "training_losses = []\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    batch_count = 0\n",
    "    \n",
    "    for batch in filtered_train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x, y = batch\n",
    "        \n",
    "        # Forward pass\n",
    "        output = alpin_deepar(x)\n",
    "        loss = alpin_deepar.loss(output, y)\n",
    "        \n",
    "        # Check for NaN\n",
    "        if torch.isnan(loss):\n",
    "            continue\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(alpin_deepar.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        batch_count += 1\n",
    "        \n",
    "        # Limit batches for quick demo\n",
    "        if batch_count >= 50:\n",
    "            break\n",
    "    \n",
    "    avg_loss = epoch_loss / max(batch_count, 1)\n",
    "    training_losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"  Epoch {epoch+1}/{MAX_EPOCHS}: Loss = {avg_loss:.4f}, Batches used = {batch_count}\")\n",
    "\n",
    "# Get filtering statistics\n",
    "filter_stats = filtered_train_dataloader.get_filtering_stats()\n",
    "print(\"\\nBatchCP Filtering Statistics:\")\n",
    "print(f\"  Total batches processed: {filter_stats['total_batches']}\")\n",
    "print(f\"  Filtered out: {filter_stats['filtered_batches']} ({filter_stats['filter_ratio']*100:.1f}%)\")\n",
    "print(f\"  Batches used: {filter_stats['kept_batches']}\")\n",
    "\n",
    "print(\"\\nALPIN-Enhanced training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ALPIN-enhanced predictions\n",
    "alpin_deepar.eval()\n",
    "\n",
    "alpin_predictions = alpin_deepar.predict(val_dataloader, return_y=True, mode=\"prediction\")\n",
    "\n",
    "alpin_preds = alpin_predictions.output\n",
    "alpin_actuals = alpin_predictions.y[0]\n",
    "\n",
    "print(f\"ALPIN predictions shape: {alpin_preds.shape}\")\n",
    "print(f\"ALPIN actuals shape: {alpin_actuals.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ALPIN-enhanced metrics\n",
    "alpin_metrics = calculate_forecast_metrics(alpin_preds, alpin_actuals)\n",
    "print(\"ALPIN-Enhanced DeepAR Metrics:\")\n",
    "print(f\"  MAE:  {alpin_metrics['MAE']:.4f}\")\n",
    "print(f\"  RMSE: {alpin_metrics['RMSE']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ALPIN-enhanced forecast example\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "sample_idx = 0\n",
    "pred_sample = alpin_preds[sample_idx].cpu().numpy()\n",
    "actual_sample = alpin_actuals[sample_idx].cpu().numpy()\n",
    "\n",
    "time_axis = np.arange(len(actual_sample))\n",
    "\n",
    "ax.plot(time_axis, actual_sample, 'o-', color='#2E86AB', label='Actual', markersize=4)\n",
    "ax.plot(time_axis, pred_sample, 's-', color='#27AE60', label='ALPIN-Enhanced Forecast', markersize=4)\n",
    "\n",
    "ax.set_xlabel('Forecast Horizon')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title('ALPIN-Enhanced DeepAR: Example Forecast vs Actual', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, linestyle=':', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison & Analysis\n",
    "\n",
    "Compare the performance of baseline DeepAR vs ALPIN-enhanced DeepAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['MAE', 'RMSE'],\n",
    "    'Baseline DeepAR': [f\"{baseline_metrics['MAE']:.4f}\", f\"{baseline_metrics['RMSE']:.4f}\"],\n",
    "    'ALPIN-Enhanced': [f\"{alpin_metrics['MAE']:.4f}\", f\"{alpin_metrics['RMSE']:.4f}\"],\n",
    "})\n",
    "\n",
    "# Calculate improvement\n",
    "mae_improvement = (baseline_metrics['MAE'] - alpin_metrics['MAE']) / baseline_metrics['MAE'] * 100\n",
    "rmse_improvement = (baseline_metrics['RMSE'] - alpin_metrics['RMSE']) / baseline_metrics['RMSE'] * 100\n",
    "\n",
    "comparison_df['Improvement'] = [f\"{mae_improvement:+.2f}%\", f\"{rmse_improvement:+.2f}%\"]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FORECAST ACCURACY COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "display(comparison_df)\n",
    "print(\"\\n(Positive improvement = ALPIN-Enhanced is better)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Side-by-side metric comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "metrics = ['MAE', 'RMSE']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "baseline_vals = [baseline_metrics['MAE'], baseline_metrics['RMSE']]\n",
    "alpin_vals = [alpin_metrics['MAE'], alpin_metrics['RMSE']]\n",
    "\n",
    "bars1 = ax.bar(x - width/2, baseline_vals, width, label='Baseline DeepAR', color='#E74C3C', edgecolor='white')\n",
    "bars2 = ax.bar(x + width/2, alpin_vals, width, label='ALPIN-Enhanced', color='#27AE60', edgecolor='white')\n",
    "\n",
    "ax.set_ylabel('Error')\n",
    "ax.set_title('Forecast Accuracy: Baseline vs ALPIN-Enhanced DeepAR', fontweight='bold', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics, fontsize=12)\n",
    "ax.legend(loc='upper right', fontsize=11)\n",
    "ax.bar_label(bars1, fmt='%.3f', padding=3, fontsize=10)\n",
    "ax.bar_label(bars2, fmt='%.3f', padding=3, fontsize=10)\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.grid(axis='y', linestyle=':', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side forecast comparison\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "sample_idx = 0\n",
    "baseline_sample = baseline_preds[sample_idx].cpu().numpy()\n",
    "alpin_sample = alpin_preds[sample_idx].cpu().numpy()\n",
    "actual_sample = baseline_actuals[sample_idx].cpu().numpy()  # Same actuals\n",
    "time_axis = np.arange(len(actual_sample))\n",
    "\n",
    "# Top: Baseline\n",
    "axes[0].plot(time_axis, actual_sample, 'o-', color='#2C3E50', label='Actual', markersize=5)\n",
    "axes[0].plot(time_axis, baseline_sample, 's-', color='#E74C3C', label='Baseline Forecast', markersize=5)\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].set_title('Baseline DeepAR Forecast', fontweight='bold')\n",
    "axes[0].legend(loc='upper right')\n",
    "axes[0].grid(True, linestyle=':', alpha=0.5)\n",
    "axes[0].spines['top'].set_visible(False)\n",
    "axes[0].spines['right'].set_visible(False)\n",
    "\n",
    "# Bottom: ALPIN-Enhanced\n",
    "axes[1].plot(time_axis, actual_sample, 'o-', color='#2C3E50', label='Actual', markersize=5)\n",
    "axes[1].plot(time_axis, alpin_sample, 's-', color='#27AE60', label='ALPIN-Enhanced Forecast', markersize=5)\n",
    "axes[1].set_xlabel('Forecast Horizon')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].set_title('ALPIN-Enhanced DeepAR Forecast (BatchCP)', fontweight='bold')\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].grid(True, linestyle=':', alpha=0.5)\n",
    "axes[1].spines['top'].set_visible(False)\n",
    "axes[1].spines['right'].set_visible(False)\n",
    "\n",
    "plt.suptitle('Forecast Comparison: Example Prediction', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchCP Filtering Statistics Summary\n",
    "print(\"=\" * 60)\n",
    "print(\"BATCHCP FILTERING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDetected Changepoints:\")\n",
    "for sid, cps in detected_changepoints.items():\n",
    "    print(f\"  {sid}: {len(cps)} changepoints at indices {cps}\")\n",
    "\n",
    "print(f\"\\nFiltering Configuration:\")\n",
    "print(f\"  Encoder window: {MAX_ENCODER_LENGTH} samples\")\n",
    "print(f\"  Tolerance margin: ±{CP_TOLERANCE} samples\")\n",
    "\n",
    "print(f\"\\nFiltering Results:\")\n",
    "print(f\"  Total batches seen: {filter_stats['total_batches']}\")\n",
    "print(f\"  Batches filtered out: {filter_stats['filtered_batches']} ({filter_stats['filter_ratio']*100:.1f}%)\")\n",
    "print(f\"  Clean batches used: {filter_stats['kept_batches']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT CONCLUSIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if alpin_metrics['MAE'] < baseline_metrics['MAE']:\n",
    "    print(\"\\n✅ ALPIN-Enhanced DeepAR OUTPERFORMS Baseline\")\n",
    "    print(f\"   MAE improved by {abs(mae_improvement):.2f}%\")\n",
    "    print(f\"   RMSE improved by {abs(rmse_improvement):.2f}%\")\n",
    "    print(\"\\n   The BatchCP method successfully reduced forecast error by\")\n",
    "    print(\"   filtering out training batches that span regime changes.\")\n",
    "else:\n",
    "    print(\"\\n⚠️ ALPIN-Enhanced DeepAR shows similar or worse performance\")\n",
    "    print(f\"   MAE difference: {mae_improvement:.2f}%\")\n",
    "    print(f\"   RMSE difference: {rmse_improvement:.2f}%\")\n",
    "    print(\"\\n   Possible reasons:\")\n",
    "    print(\"   - Synthetic data may not have strong regime effects\")\n",
    "    print(\"   - Training data reduced too much by filtering\")\n",
    "    print(\"   - Model needs more epochs to converge\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Key Findings:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"1. ALPIN detected {sum(len(cps) for cps in detected_changepoints.values())} changepoints across {N_SIGNALS} signals\")\n",
    "print(f\"2. BatchCP filtered {filter_stats['filter_ratio']*100:.1f}% of training batches\")\n",
    "print(f\"3. Baseline MAE: {baseline_metrics['MAE']:.4f}, ALPIN-Enhanced MAE: {alpin_metrics['MAE']:.4f}\")\n",
    "print(f\"4. Baseline RMSE: {baseline_metrics['RMSE']:.4f}, ALPIN-Enhanced RMSE: {alpin_metrics['RMSE']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. **Generated synthetic signals** with known changepoints using ALPIN's data generator\n",
    "2. **Trained ALPIN** to detect changepoints in the synthetic data\n",
    "3. **Trained baseline DeepAR** on all available training data\n",
    "4. **Implemented BatchCP filtering** via `ChangePointAwareDataLoader`\n",
    "5. **Trained ALPIN-enhanced DeepAR** using only \"clean\" batches without changepoints\n",
    "6. **Compared forecast accuracy** between both approaches\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **BatchCP is a simple but effective preprocessing technique** that can improve forecast quality by avoiding regime-spanning training samples\n",
    "- **ALPIN provides accurate changepoint detection** that enables the BatchCP filtering\n",
    "- **The improvement depends on data characteristics**: signals with strong regime changes benefit most\n",
    "- **Trade-off**: Filtering reduces training data, which may hurt if changepoints are very frequent\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Test on real financial data with actual regime changes\n",
    "- Compare with other changepoint-aware methods\n",
    "- Experiment with different filtering tolerances\n",
    "- Evaluate on longer forecast horizons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
