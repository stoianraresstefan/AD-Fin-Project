{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALPIN Training Workflow\n",
    "\n",
    "This notebook demonstrates the comprehensive training workflow for the ALPIN (Adaptive Learning of Penalty for INference) algorithm. We will cover:\n",
    "1. Synthetic data generation\n",
    "2. Model training with cross-validation\n",
    "3. Performance evaluation using multiple metrics\n",
    "4. Comparison of labeling protocols (Protocol I vs. Protocol II)\n",
    "5. Hyperparameter sweep visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from alpin import ALPIN\n",
    "from alpin.data.synthetic import generate_synthetic_signals\n",
    "from alpin.metrics import evaluate_all\n",
    "from alpin.experiments.sweep import sweep_beta\n",
    "from alpin.visualization import plot_signal, plot_sweep_results\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Synthetic Data\n",
    "\n",
    "We generate 100 synthetic signals following the specifications in the EUSIPCO 2017 paper. Each signal has 500 samples and between 3 to 7 changepoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_signals = 100\n",
    "n_samples = 500\n",
    "\n",
    "signals, truths = generate_synthetic_signals(\n",
    "    n_signals=n_signals, \n",
    "    n_samples=n_samples, \n",
    "    noise_std=1.0, \n",
    "    seed=42,\n",
    "    protocol=\"I\"\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(signals)} signals with Protocol I labels.\")\n",
    "plot_signal(signals[0], truths[0], title=\"Example Synthetic Signal (Protocol I)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training with Cross-Validation\n",
    "\n",
    "We use 10-fold cross-validation to evaluate the stability of the learned penalty parameter $\\beta$ and the model's generalization performance.\n",
    "\n",
    "### Why Cross-Validation?\n",
    "Cross-validation ensures that our evaluation is not biased by a specific train/test split. For each fold, we:\n",
    "1. Train ALPIN on 90% of the data to learn $\\beta_{opt}$.\n",
    "2. Evaluate the learned $\\beta_{opt}$ on the remaining 10% (test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "cv_results = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(tqdm(kf.split(signals), total=10, desc=\"CV Folds\")):\n",
    "    # Split data\n",
    "    train_signals = [signals[i] for i in train_idx]\n",
    "    train_truths = [truths[i] for i in train_idx]\n",
    "    test_signals = [signals[i] for i in test_idx]\n",
    "    test_truths = [truths[i] for i in test_idx]\n",
    "    \n",
    "    # Train model\n",
    "    model = ALPIN()\n",
    "    model.fit(train_signals, train_truths)\n",
    "    beta_fold = model.beta_opt\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    fold_metrics = []\n",
    "    for s, t in zip(test_signals, test_truths):\n",
    "        pred = model.predict(s)\n",
    "        m = evaluate_all(pred, t, len(s), tolerance=10)\n",
    "        fold_metrics.append(m)\n",
    "    \n",
    "    # Average metrics for this fold\n",
    "    avg_metrics = pd.DataFrame(fold_metrics).mean().to_dict()\n",
    "    avg_metrics['beta'] = beta_fold\n",
    "    avg_metrics['fold'] = fold\n",
    "    cv_results.append(avg_metrics)\n",
    "\n",
    "df_cv = pd.DataFrame(cv_results)\n",
    "display(df_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of CV Results\n",
    "The table below shows the average performance across all 10 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = df_cv.drop(columns=['fold']).agg(['mean', 'std']).T\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Protocol I vs. Protocol II\n",
    "\n",
    "The paper defines two labeling protocols:\n",
    "- **Protocol I**: All changepoints are labeled.\n",
    "- **Protocol II**: Only changepoints with a jump amplitude $|\\Delta| > 3$ are labeled.\n",
    "\n",
    "We expect Protocol II to result in a larger $\\beta$ (higher penalty) because it ignores smaller jumps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocols = [\"I\", \"II\"]\n",
    "protocol_comparison = []\n",
    "\n",
    "for p in protocols:\n",
    "    # Generate data for this protocol\n",
    "    p_signals, p_truths = generate_synthetic_signals(n_signals=50, seed=123, protocol=p)\n",
    "    \n",
    "    # Train on full set\n",
    "    model = ALPIN()\n",
    "    model.fit(p_signals, p_truths)\n",
    "    \n",
    "    protocol_comparison.append({\n",
    "        \"Protocol\": p,\n",
    "        \"Learned Beta\": model.beta_opt,\n",
    "        \"Avg. Changepoints\": np.mean([len(t) for t in p_truths])\n",
    "    })\n",
    "\n",
    "df_proto = pd.DataFrame(protocol_comparison)\n",
    "display(df_proto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Sweep Visualization\n",
    "\n",
    "We can visualize how the choice of $\\beta$ affects different metrics using a grid search (sweep)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_range = np.logspace(0, 3, 20)\n",
    "sweep_results = sweep_beta(signals[:20], truths[:20], beta_range, n_splits=3)\n",
    "\n",
    "# Aggregate results\n",
    "sweep_agg = sweep_results.groupby('beta').mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(sweep_agg['beta'], sweep_agg['precision'], label='Precision')\n",
    "plt.plot(sweep_agg['beta'], sweep_agg['recall'], label='Recall')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Beta')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision-Recall Tradeoff')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(sweep_agg['beta'], sweep_agg['rand_index'], label='Rand Index', color='green')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Beta')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Rand Index vs. Beta')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In this notebook, we demonstrated that ALPIN successfully learns an optimal penalty parameter $\\beta$ that balances precision and recall. We also observed how different labeling protocols influence the learned parameter, with Protocol II leading to a more conservative model that ignores small jumps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
